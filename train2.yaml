$schema: https://componentsdk.azureedge.net/jsonschema/DistributedComponent.json
#$schema: https://azuremlschemas.azureedge.net/latest/commandJob.schema.json
#$schema: http://azureml/sdk-2-0/DistributedComponent.json
# Metadata
name: llamax_training
version: 0.0.28
display_name: llamax_training
type: DistributedComponent
#type: DistributedComponent@1-preview
description: Run llamax training on training data
tags: {author: caxu@microsoft.com}
is_deterministic: true
# Interface
inputs:
  model_name_or_path:
    type: path
    optional: false
    description: Training model path.
  data_path:
    type: path
    optional: false
    description: Training dataset path
  model_max_length:
    type: integer
    optional: true
    default: 2048
    description: model max length
  num_nodes:
    type: integer
    optional: true
    description: number of training nodes
  num_train_epochs:
    type: integer
    optional: true
    default: 3
    description: number of training epochs
  per_device_train_batch_size:
    type: integer
    optional: true
    description: per train batch size
  per_device_eval_batch_size:
    type: integer
    optional: true
    default: 1
    description: per eval batch size
  gradient_accumulation_steps:
    type: integer
    optional: true
    default: 1
    description: gradient_accumulation_steps
  evaluation_strategy:
    type: string
    optional: true
    default: "no"
    description: evaluation_strategy
  save_strategy:
    type: string
    optional: true
    default: "steps"
    description: save_strategy
  save_total_limit:
    type: integer
    optional: true
    description: save_total_limit
  learning_rate:
    type: number
    optional: true
    default: 2e-5
    description: learning_rate
  weight_decay:
    type: number
    optional: true
    default: 0.0
    description: learning_rate
  warmup_steps:
    type: integer
    optional: true
    default: 30
    description: warmup_steps
  save_steps:
    type: integer
    optional: true
    description: warmup_steps
  logging_steps:
    type: integer
    optional: true
    default: 2
    description: logging_steps
  lr_scheduler_type:
    type: string
    optional: true
    default: "cosine"
    description: lr_scheduler_type
  fp16:
    description: Use bfloat16 for the model weights.
    optional: true
    default: true
    type: boolean
  gradient_checkpointing:
    description: gradient_checkpointing
    optional: true
    default: true
    type: boolean
  deepspeed:
    type: path
    optional: true
    default: configs/deepspeed_config_transformers4.31.json
    description: deepspeed configs
outputs: 
  output_dir: 
    description: Path to save the checkpoints
    type: path
    optional: false
meta: 
  requireGpu: true

environment:
  #name: azureml://locations/westus3/workspaces/76b81224-c4d6-4772-aa28-cebca6186015/environments/vicuna_final6/versions/1
  docker:
    #image: pytorch/pytorch:2.0.1-cuda11.7-cudnn8-devel
    image: indexserveregistry.azurecr.io/deepspeed:latest_torch18-cuda11.1-nccl_bootstrap_tag
  conda_file:
    name: project_environment
    channels:
      - defaults
    dependencies:
      - git
      - python=3.10.11
      - packaging
      - pytest      
      - pip:        
        - numpy
        - rouge_score
        - fire
        - openai
        - transformers==4.31.0        
        - accelerate        
        - sentencepiece
        - wandb        
        - deepspeed==0.10.0
        - tensorboardX
        - azureml-core==1.51.0
        - mlflow
        - azureml-mlflow
os: Linux


launcher:
  type: torch.distributed
  additional_arguments: > 
    pip install transformers==4.31.0;
    pip install numpy;
    pip install rouge_score;
    pip install fire;
    pip install openai;
    pip install sentencepiece;
    pip install wandb;
    pip install deepspeed==0.10.0;
    pip install accelerate;
    pip install tensorboardX;
    deepspeed --num_nodes 6 --num_gpus 4 train_freeform_multiturn.py  --model_name_or_path {inputs.model_name_or_path} --data_path {inputs.data_path} --output_dir {outputs.output_dir}  --model_max_length 2048 --num_train_epochs 3  --per_device_train_batch_size 8 --per_device_eval_batch_size 1  --gradient_accumulation_steps 1   --evaluation_strategy "no"     --save_strategy "steps"     --save_steps 100     --save_total_limit 6     --learning_rate 2e-5 --weight_decay 0.     --warmup_steps 20     --logging_steps 2     --lr_scheduler_type "cosine"     --report_to "tensorboard"     --gradient_checkpointing True   --deepspeed configs/deepspeed_config_transformers4.31.json     --fp16 True 




# command: >-
#     deepspeed src/train_freeform_multiturn.py
#       # $[[--num_nodes ${{inputs.num_nodes}}]]
#       $[[--model_name_or_path ${{inputs.model_name_or_path}}]]
#       $[[--data_path ${{inputs.data_path}}]]
#       $[[--model_max_length ${{inputs.model_max_length}}]]
#       $[[--num_train_epochs ${{inputs.num_train_epochs}}]]
#       $[[--per_device_train_batch_size ${{inputs.per_device_train_batch_size}}]]
#       $[[--per_device_eval_batch_size ${{inputs.per_device_eval_batch_size}}]]
#       $[[--gradient_accumulation_steps ${{inputs.gradient_accumulation_steps}}]]
#       $[[--evaluation_strategy ${{inputs.evaluation_strategy}}]]
#       $[[--save_strategy ${{inputs.save_strategy}}]]
#       $[[--save_steps ${{inputs.save_steps}}]]
#       $[[--save_total_limit ${{inputs.save_total_limit}}]]
#       $[[--learning_rate ${{inputs.learning_rate}}]]
#       $[[--weight_decay ${{inputs.weight_decay}}]]
#       $[[--warmup_steps ${{inputs.warmup_steps}}]]
#       $[[--logging_steps ${{inputs.logging_steps}}]]
#       $[[--lr_scheduler_type ${{inputs.lr_scheduler_type}}]]
#       $[[--gradient_checkpointing ${{inputs.gradient_checkpointing}}]]
#       $[[--deepspeed ${{inputs.deepspeed}}]] 
#       $[[--fp16 ${{inputs.fp16}}]]  
#       --output_dir ${{outputs.output_dir}}

# distribution:
#   type: pytorch

